#!/usr/bin/env python3

import json, math, os, re, shutil, subprocess, sys, threading, time
from pathlib import Path
from functools import cache

# Constants
USER = "ubuntu"
WORKSPACE = f"/home/{USER}/workspace"
RUN_DIR = f"{WORKSPACE}/rondb-run"
NUM_CLUSTER_CONN_MYSQLD=1
NUM_CLUSTER_CONN_RDRS=2
NUM_CLUSTER_CONN_BENCH=1
config_required_strings = [
    "bench_instance_type",
    "cpu_platform",
    "glibc_version",
    "grafana_instance_type",
    "mysqld_instance_type",
    "ndb_mgmd_instance_type",
    "ndbmtd_instance_type",
    "prometheus_instance_type",
    "rdrs_instance_type",
    "region",
    "rondb_version",
]
config_required_ints = [
    "bench_count",
    "bench_disk_size",
    "mysqld_count",
    "mysqld_disk_size",
    "ndbmtd_count",
    "ndbmtd_disk_size",
    "num_azs",
    "prometheus_disk_size",
    "rdrs_count",
    "rdrs_disk_size",
    "rondb_replicas",
]
config_vars_not_in_tfvars = [
    "rondb_replicas",
    "rondb_version",
    "glibc_version",
]

# Node types, number of NDB NodeIds, and color
node_types = [
    ("ndb_mgmd",   1,                       "48;2;21;149;230;38;5;16"),
    ("ndbmtd",     1,                       "48;2;3;193;148;38;5;16"),
    ("mysqld",     NUM_CLUSTER_CONN_MYSQLD, "48;2;255;227;71;38;5;16"),
    ("rdrs",       NUM_CLUSTER_CONN_RDRS,   "48;2;233;125;103;38;5;16"),
    ("prometheus", 0,                       "48;2;178;141;248;38;5;16"),
    ("grafana",    0,                       "48;2;239;161;248;38;5;16"),
    ("bench",      NUM_CLUSTER_CONN_BENCH,  "48;2;255;178;75;38;5;16"),
]
import sys
stdout_is_tty=sys.stdout.isatty()
def color(s, c, b=""): return f"\033[{c}m{b+s+b}\033[0m" if stdout_is_tty else s
def red(s): return color(s, '1;31')
def green(s): return color(s, '1;32')
def lnk(uri):
    if not stdout_is_tty: return uri
    # Make URI bold underlined, and if supported, clickable.
    return f"\033]8;;{uri}\033\\\033[1;4m{uri}\033[0m\033]8;;\033\\"

# Nodes
@cache
def all_nodes():
    nodes = []
    nodeid = 0
    for node_type, num_nodeids, clr in node_types:
        public_ips = get_tf_output(f'{node_type}_public_ips')
        private_ips = get_tf_output(f'{node_type}_private_ips')
        assert len(public_ips) == len(private_ips), "IP count mismatch"
        for i in range(len(public_ips)):
            nodes.append(Node(node_type,
                              i,
                              public_ips[i],
                              private_ips[i],
                              [(nodeid:=nodeid+1) for _ in range(num_nodeids)],
                              clr))
    return nodes
@cache
def max_node_name_len(): return max(len(n.name()) for n in all_nodes())
def nodes_of_type(node_type):
    return [n for n in all_nodes() if n.type == node_type]
def node_count(node_type): return len(nodes_of_type(node_type))
class Node:
    def __init__(self, type, idx, public_ip, private_ip, ndb_nodeids, clr):
        self.type = type
        self.idx = idx
        self.public_ip = public_ip
        self.private_ip = private_ip
        self.ndb_nodeids = ndb_nodeids
        self.clr = clr
    def name(self):
        ret = self.type
        if node_count(self.type) > 1:
            ret += f"_{self.idx}"
        return ret
    def tui_name(self):
        name = self.name()
        suffix = ' ' * (max_node_name_len() - len(name))
        return color(name, self.clr, " ") + suffix
def first(node_type):
    for node in all_nodes():
        if node.type == node_type:
            return node
    raise ValueError(f"No node of type {node_type} found")
def nodes_from_expression(expr):
    # expr is None for all nodes, or a comma-separated list of node names or
    # node types.
    if expr == None: return all_nodes()
    ret = []
    for node_desc in expr.split(","):
        described_nodes = [node for node in all_nodes()
                           if node.name() == node_desc
                           or node.type == node_desc ]
        if len(described_nodes) == 0:
            die(f"'{node_desc}' doesn't match any node name or type."
                " Try ./cluster_ctl list")
        ret += described_nodes
    # Deduplicate and order
    ret = [node for node in all_nodes() if node in ret ]
    return ret

# Config
@cache
def config():
    # Import config from ./config.py, validate and return
    import config as config_module
    c = config_module.config
    for var in config_required_strings + config_required_ints:
        if var not in c:
            die(f"Config variable '{var}' not found in config.py.")
        if var in config_required_strings and not (
                isinstance(c[var], str) and c[var].strip()):
            die(f"Config variable '{var}' must be a nonempty string in"
                " config.py.")
        if var in config_required_ints and not (
                isinstance(c[var], int) and c[var] > 0):
            die(f"Config variable '{var}' must be a positive integer in"
                " config.py.")
    if c['ndbmtd_count'] % c['rondb_replicas'] != 0:
        die(f"Error: ndbmtd_count ({c['ndbmtd_count']}) must be divisible by"
            " rondb_replicas ({c['rondb_replicas']}).")
    if c['cpu_platform'] not in ['arm64_v8', 'x86_64']:
        die(f"Error: cpu_platform must be either 'arm64_v8' or 'x86_64',"
            f" but got '{c['cpu_platform']}' from config.py.")
    for var in c:
        if var not in config_required_strings + config_required_ints:
            die(f"Unexpected variable '{var}' in config.py.")
    return c
def get_config(var_name):
    c = config()
    if var_name in c:
        return c[var_name]
    die(f"Error: config variable '{var_name}' not found.")

# Terraform outputs
has_read_tf_output = False
@cache
def tf_output():
    global has_read_tf_output
    tf_output_file = Path("tf_output")
    state_file = Path("terraform.tfstate")
    if not state_file.exists():
        die("Error: terraform.tfstate does not exist."+
            " Perhaps you forgot to run './cluster_ctl terraform'?")
    if not (tf_output_file.exists() and
            state_file.exists() and
            state_file.stat().st_mtime < tf_output_file.stat().st_mtime):
        proc = subprocess.run("terraform output -json > tf_output", shell=True)
        if proc.returncode != 0:
            die(f"Error running terraform output: {proc.stdout + proc.stderr}")
    has_read_tf_output = True
    return json.loads(tf_output_file.read_text())
def get_tf_output(output_name):
    return tf_output()[output_name]['value']

@cache
def get_rnd_cache():
    secret_file = Path("secrets.json")
    if not secret_file.exists():
        secret_file.write_text(json.dumps({
            "gui_secret": os.urandom(10).hex(),
            "unique_suffix": os.urandom(8).hex(),
            "demo_mysql_pw": os.urandom(10).hex(),
        }))
    return json.loads(secret_file.read_text())

def get_gui_secret(): return get_rnd_cache()["gui_secret"]
def get_demo_mysql_pw(): return get_rnd_cache()["demo_mysql_pw"]
def get_unique_suffix(): return get_rnd_cache()["unique_suffix"]
def get_key_name(): return f"rondb_bench_key_{get_unique_suffix()}"

# Util functions
def w(name, *content): Path(name).write_text("\n".join(content) + "\n")
def mkdir(p): Path(p).mkdir(parents=True, exist_ok=True)

# Generate ./config_files
def generate_config_files():
    # Clean directory
    shutil.rmtree("config_files", ignore_errors=True)
    mkdir("config_files")
    # Extract some values
    USER="ubuntu"
    WORKSPACE=f"/home/{USER}/workspace"
    CONFIG_FILES=f"/home/{USER}/config_files"
    RUN_DIR=f"{WORKSPACE}/rondb-run"
    NDB_MGMD_PRI = first('ndb_mgmd').private_ip
    NO_OF_REPLICAS = get_config('rondb_replicas')
    NUM_AZS = get_config('num_azs')
    RONDB_VERSION = get_config('rondb_version')
    GLIBC_VERSION = get_config('glibc_version')
    CPU_PLATFORM = get_config('cpu_platform')
    TARBALL_NAME = f"rondb-{RONDB_VERSION}-linux-glibc{GLIBC_VERSION}-{CPU_PLATFORM}.tar.gz"
    RDRS_IP= (get_tf_output('rdrs_nlb_dns')
              if node_count('rdrs') > 1
              else first('rdrs').private_ip)
    GUI_SECRET = get_gui_secret()
    # Generate shell_vars
    shell_vars = {
        "USER": USER,
        "WORKSPACE": WORKSPACE,
        "CONFIG_FILES": CONFIG_FILES,
        "RUN_DIR": RUN_DIR,
        "TARBALL_NAME": TARBALL_NAME,
        "NUM_AZS": f"{NUM_AZS}",
        "NDB_MGMD_PRI": NDB_MGMD_PRI,
        "MYSQLD_PUB_1": first('mysqld').public_ip,
        "MYSQLD_PRI_1": first('mysqld').private_ip,
        "RDRS_URI": f"http://{RDRS_IP}:4406",
        "RONDIS_LB": f"{get_tf_output('rondis_nlb_dns')}:6379",
        "RDRS_PRI_1": first('rdrs').private_ip,
        "BENCH_PRI_1": first('bench').private_ip,
        "DEMO_MYSQL_PW": get_demo_mysql_pw(),
        "GRAFANA_PRI_1": first('grafana').private_ip,
        "GUI_SECRET": GUI_SECRET,
    }
    w("config_files/shell_vars",
      *[f"export {var}={value}" for var, value in shell_vars.items()])
    # Generate config.ini
    configini = [
        "[NDB_MGMD DEFAULT]",
        f"DataDir={RUN_DIR}/ndb_mgmd/data",
        "",
        "[NDBD DEFAULT]",
        "AutomaticThreadConfig=true",
        "AutomaticMemoryConfig=true",
        "MaxDMLOperationsPerTransaction=100000",
        "MaxNoOfConcurrentOperations=100000",
        f"FileSystemPath={RUN_DIR}/ndbmtd/ndb_data",
        f"FileSystemPathDD={RUN_DIR}/ndbmtd/ndb_disk_columns",
        f"DataDir={RUN_DIR}/ndbmtd/data",
        "",
        f"NoOfReplicas={NO_OF_REPLICAS}",
        "PartitionsPerNode=4",
    ]
    node_type_configini_header = {
        "ndb_mgmd":   "[NDB_MGMD]",
        "ndbmtd":     "[NDBD]",
        "mysqld":     "[MYSQLD]",
        "rdrs":       "[API]",
        "bench":      "[API]",
    }
    for node in all_nodes():
        if not node.type in node_type_configini_header: continue
        configini += [""]
        for nodeid in node.ndb_nodeids:
            configini += [
                node_type_configini_header[node.type],
                f"# {node.name()}",
                f"HostName={node.private_ip}",
                f"NodeId={nodeid}",
                f"LocationDomainId={(node.idx % NUM_AZS) + 1}"
            ]
    w("config_files/config.ini", *configini)
    # Generate my.cnf
    w("config_files/my.cnf",
        "[mysqld]",
        "ndbcluster",
        "user=root",
        f"basedir={WORKSPACE}/rondb",
        f"datadir={RUN_DIR}/mysqld/data",
        f"log_error={RUN_DIR}/mysqld/data/mysql-error.log",
        "log_error_verbosity=3",
        "",
        "[mysql_cluster]",
        f"ndb-connectstring={NDB_MGMD_PRI}",
      )
    for node in all_nodes():
        w(f"config_files/nodeinfo_{node.name()}",
          f"NODEINFO_ROLE={node.type}",
          f"NODEINFO_IDX={node.idx}",
          f"NODEINFO_NODEIDS=\"{' '.join(map(str, node.ndb_nodeids))}\"",
          )
    # Generate rdrs_*.json
    for node in nodes_of_type("rdrs"):
        w(f"config_files/rdrs_{node.idx}.json",
          json.dumps({
              "RonDB": {
                  "Mgmds": [{"IP": NDB_MGMD_PRI}],
                  "ConnectionPoolSize": 2,
                  "NodeIDs": node.ndb_nodeids,
              },
              "REST": {
                  "NumThreads": 64,
                  "UseCompression": False,
                  "UseSingleTransaction": False,
              },
              "Rondis": {
                  "Enable": True,
                  "NumThreads": 32,
                  "NumDatabases": 2,
                  "Databases": [
                      {
                          "Index": 0,
                          "FastHCOUNT": True,
                      },
                      {
                          "Index": 1,
                          "FastHCOUNT": False,
                      },
                  ],
              },
              "Security": {
                  "APIKey": {
                      "UseHopsworksAPIKeys": False,
                  },
              },
          }, indent=4))
    # Generate prometheus.yml
    prometheus_yml = [
        "global:",
        "  scrape_interval: 15s",
        "scrape_configs:",
        "  - job_name: 'rdrs'",
        "    static_configs:",
    ]
    for node in nodes_of_type("rdrs"):
        prometheus_yml += [
            f"      - targets: ['{node.private_ip}:4406']",
            "        labels:",
            f"          instance: '{node.name()}'",
        ]
    prometheus_yml += [
        "  - job_name: 'mysqld'",
        "    static_configs:",
    ]
    for node in nodes_of_type("mysqld"):
        prometheus_yml += [
            f"      - targets: ['{node.private_ip}:9104']",
            "        labels:",
            f"          instance: '{node.name()}'",
        ]
    prometheus_yml += [
        "  - job_name: 'linux'",
        "    static_configs:",
    ]
    for node in all_nodes():
        prometheus_yml += [
            f"      - targets: ['{node.private_ip}:9100']",
            "        labels:",
            f"          instance: '{node.name()}'",
            f"          instance_type: '{node.type}'",
        ]
    w("config_files/prometheus.yml", *prometheus_yml)
    # Generate grafana/
    mkdir("config_files/grafana/dashboards")
    mkdir("config_files/grafana/datasources")
    mkdir("config_files/grafana/plugins")
    mkdir("config_files/grafana/alerting")
    w("config_files/grafana/grafana.ini",
      "[paths]",
      f"data = {RUN_DIR}/grafana",
      f"logs = {RUN_DIR}/grafana/logs",
      f"provisioning = {CONFIG_FILES}/grafana",
      "[auth]",
      "disable_login_form = true",
      "[auth.anonymous]",
      "enabled = true",
      "org_role = Viewer",
      "[server]",
      f"root_url = http://{first('bench').public_ip}:8080/grafana/",
      "serve_from_sub_path = true",
      "[live]",
      f"allowed_origins = http://{first('bench').public_ip}:8080",
      )
    w("config_files/grafana/datasources/prometheus.yaml",
      "apiVersion: 1",
      "",
      "datasources:",
      "  - name: Prometheus",
      "    type: prometheus",
      "    access: proxy",
      f"    url: http://{first('prometheus').private_ip}:9090",
      "    isDefault: true",
      "    uid: prometheus",
      # Dashboards copied from hopsworks-helm use a different data source uid.
      # By defining a second data source with the same endpoint, we can support
      # both without changing the dashboards.
      "  - name: Prometheus (alias)",
      "    type: prometheus",
      "    access: proxy",
      f"    url: http://{first('prometheus').private_ip}:9090",
      "    isDefault: false",
      "    uid: PBFA97CFB590B2093",
      )
    w("config_files/grafana/dashboards/dashboards.yaml",
      "apiVersion: 1",
      "",
      "providers:",
      "  - name: 'RonDB dashboards'",
      "    orgId: 1",
      "    folder: ''",
      "    folderUid: ''",
      "    type: file",
      "    options:",
      f"      path: {CONFIG_FILES}/grafana/dashboards",
      )
    shutil.copytree("dashboards", "config_files/grafana/dashboards",
                    dirs_exist_ok=True)
    # nginx config to expose grafana, locust and demo UI on the first bench
    # node. This implements a simple cookie-based authentication. An
    # unauthenticated user receives 403 for all requests except
    # /secret=SECRET/path, which will set the authentication cookie and redirect
    # to /path. location / is proxied to the demo UI without authentication, on
    # purpose.
    w("config_files/nginx.conf",
      f'error_log {RUN_DIR}/nginx/nginx_error.log error;',
      f'pid {RUN_DIR}/nginx/nginx.pid;',
       'events {',
       '  worker_connections 4096;',
       '}',
       'http {',
       '  map $http_upgrade $connection_upgrade {',
       '    ""      close;',
       '    default upgrade;',
       '  }',
       '  map $http_cookie $gui_secret {',
       '      "~X-AUTH=([^;]+)" $1;',
       '      default "";',
       '  }',
      f'  include {CONFIG_FILES}/nginx-dynamic.conf;',
      f'  client_body_temp_path {RUN_DIR}/nginx/client_temp;',
      f'  proxy_temp_path       {RUN_DIR}/nginx/proxy_temp;',
      f'  fastcgi_temp_path     {RUN_DIR}/nginx/fastcgi_temp;',
      f'  uwsgi_temp_path       {RUN_DIR}/nginx/uwsgi_temp;',
      f'  scgi_temp_path        {RUN_DIR}/nginx/scgi_temp;',
       '  server {',
      f'    access_log {RUN_DIR}/nginx/nginx_access.log;',
       '    listen 8080;',
       '    location ~ ^/secret=([0-9a-f]+)(/.*)$ {',
       '      add_header Set-Cookie "X-AUTH=$1; Path=/";',
       '      return 302 $2;',
       '    }',
       '    location /grafana {',
       '      if ($secret_is_valid = 0) {',
       '        return 403;',
       '      }',
       '      proxy_http_version 1.1;',
       '      proxy_set_header Upgrade $http_upgrade;',
       '      proxy_set_header Connection $connection_upgrade;',
      f'      proxy_pass http://{first("grafana").private_ip}:3000;',
       '    }',
       '    location /locust/ {',
       '      if ($secret_is_valid = 0) {',
       '        return 403;',
       '      }',
       '      proxy_http_version 1.1;',
       '      proxy_set_header Upgrade $http_upgrade;',
       '      proxy_set_header Connection $connection_upgrade;',
       '      rewrite ^/locust/(.*)$ /$1 break;',
       '      proxy_pass http://127.0.0.1:$locust_http_port;',
       '    }',
       '    location / {',
       '      proxy_http_version 1.1;',
       '      proxy_set_header Upgrade $http_upgrade;',
       '      proxy_set_header Connection $connection_upgrade;',
       '      proxy_pass http://127.0.0.1:8000;',
       '    }',
       '  }',
       '}',
      )
    # WARNING: nginx-dynamic.conf is re-generated in scripts/python_server.py,
    # so keep the logic in sync.
    w("config_files/nginx-dynamic.conf",
       'map $gui_secret $secret_is_valid {',
      f'    "{GUI_SECRET}" 1;',
       '    default 0;',
       '}',
       'map $gui_secret $locust_http_port {',
      f'    "{GUI_SECRET}" 8089;',
       '    default 0;',
       '}',
      )

# Parallell processing and TUI
failed = False
def die(s):
    if s: print(red(s))
    exit(1)
def check_failed():
    if failed: die("Failed")
def stream_process(cmd, prefix, check=False):
    global failed
    def do_pipe(out, pipe_prefix):
        suppressed=0
        for line in out:
            if failed:
                suppressed += 1
            else:
                print(f"{pipe_prefix}{line.rstrip()}")
        out.close()
        if suppressed > 0:
                print(f"{pipe_prefix} {red('SUPPRESSED')} {suppressed} lines due to error elsewhere")
    proc = subprocess.Popen(
        cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    stdout_thread = threading.Thread(
        target=do_pipe,
        args=(proc.stdout,f"{prefix} stdout: "))
    stdout_thread.start()
    stderr_thread = threading.Thread(
        target=do_pipe,
        args=(proc.stderr, f"{prefix} stderr: "))
    stderr_thread.start()
    proc.wait()
    stdout_thread.join()
    stderr_thread.join()
    if proc.returncode == 0:
        print(f"{prefix} {green('DONE')} {' '.join(cmd)}")
    else:
        print(f"{prefix} {red('EXIT')} code {proc.returncode}, command: {' '.join(cmd)}")
        failed = True
    if check: check_failed()
def update_status(node):
    proc = subprocess.run(ssh(node, 'bash ~/scripts/status.sh'),
                          capture_output=True, text=True)
    node.status = proc.stdout.strip() if proc.returncode == 0 else None
# Allow up to 10 concurrent rsync processes to upload config files to nodes
upload_lock = threading.Semaphore(10)
# Allow up to 2 nodes to download from repo.hops.works concurrently
repo_lock = threading.Semaphore(2)
LOCK_REPO = 1
UNLOCK_REPO = 2
LOCK_UPLOAD = 3
UNLOCK_UPLOAD = 4
UPDATE_STATUS = 5
def run_for_one_node(node, cmdfun):
    cmds = cmdfun(node)
    for cmd in cmds:
        if cmd == LOCK_UPLOAD: upload_lock.acquire()
        elif cmd == UNLOCK_UPLOAD: upload_lock.release()
        elif cmd == LOCK_REPO: repo_lock.acquire()
        elif cmd == UNLOCK_REPO: repo_lock.release()
        elif cmd == UPDATE_STATUS: update_status(node)
        elif not failed: stream_process(cmd, node.tui_name())
def run_for_all(for_nodes, cmdfun):
    threads = [ threading.Thread(target=run_for_one_node, args=(node, cmdfun))
                for node in for_nodes ]
    for thread in threads: thread.start()
    for t in threads: t.join()
    check_failed()
def ssh_opts():
    return [
        "-i", f"{get_key_name()}.pem",
        "-o", "StrictHostKeyChecking=no",
        "-q"]
def ssh(node, cmd=""):
    return ["ssh", *ssh_opts(), f"{USER}@{node.public_ip}",
            *([] if cmd=="" else [cmd])]
def rsync(file, node):
    return ["rsync", "-az", "--delete", "-e", f"ssh {' '.join(ssh_opts())}",
            file, f"{USER}@{node.public_ip}:"]

# deploy command
def do_deploy(expr):
    do_terraform()
    for_nodes = nodes_from_expression(expr)
    do_install(for_nodes)
    do_start_cluster(for_nodes)

# terraform command
def do_terraform():
    if has_read_tf_output:
        die("Bug: Called do_terraform() after tf_output()")
    # Generate ./terraform.tfvars
    w("terraform.tfvars",
      *[f'{var}="{get_config(var)}"' for var in config_required_strings
        if var not in config_vars_not_in_tfvars],
      *[f'{var}={get_config(var)}' for var in config_required_ints
        if var not in config_vars_not_in_tfvars],
      f'key_name="{get_key_name()}"',
      f'unique_suffix="{get_unique_suffix()}"',
      )
    # Create AWS SSH key pair and save to a .pem file, unless it already exists.
    key_name=get_key_name()
    key_file=Path(f"{key_name}.pem")
    if not key_file.exists():
        print(f"Creating key pair {key_name}...")
        subprocess.run(f"aws ec2 create-key-pair --key-name {key_name}"
                       f" --key-type ed25519 --region {get_config('region')}"
                       f" --query KeyMaterial --output text > {key_name}.pem",
                       shell=True, check=True)
        key_file.chmod(0o600)
    if not key_file.exists() or key_file.stat().st_size == 0:
        die("Failed to create key pair.")
    # Initialize terraform if necessary
    if not Path(".terraform").exists():
        print("Running terraform init...")
        if subprocess.run(["terraform", "init"]).returncode != 0:
            die("'terraform init' failed.")
        print("Terraform initialized successfully.")
    # terraform apply
    if subprocess.run(["terraform", "apply", "-auto-approve"]).returncode != 0:
        die("'terraform apply' failed.")

# install command
def do_install(for_nodes):
    generate_config_files()
    print("Deployment preview:")
    print("--------------------------------------------------")
    for node in for_nodes:
        print(f"{node.tui_name()} -> {node.private_ip} / {node.public_ip}")
    print("--------------------------------------------------")
    print("Press Ctrl+C within 3 seconds to abort.")
    time.sleep(3)
    print("Installing...")
    run_for_all(for_nodes, lambda node: [
        ssh(node, "sudo apt-get update -y"),
        ssh(node, "sudo apt-get install -y rsync libncurses6"),
        LOCK_UPLOAD,
        rsync("./scripts", node),
        rsync("./config_files", node),
        UNLOCK_UPLOAD,
        ssh(node, f"cp config_files/nodeinfo_{node.name()} config_files/nodeinfo"),
        LOCK_REPO,
        ssh(node, "bash ~/scripts/download_tarball.sh"),
        UNLOCK_REPO,
        ssh(node, "bash ~/scripts/install.sh"),
    ])

# start command
def do_start_cluster(for_nodes):
    print("Before starting services, make sure they are stopped.")
    do_stop_cluster(for_nodes)
    print_uri = False
    for start_type, _, _ in node_types:
        nodes_this_iteration = [n for n in for_nodes if n.type == start_type]
        count = len(nodes_this_iteration)
        if count == 0: continue
        if start_type == "bench": print_uri = True
        print("Waiting 2 seconds before starting services on the" +
              (f" {count} " if count>1 else " ") +
              color(start_type, first(start_type).clr, " ") +
              f" node{'s' if count>1 else ''}...")
        time.sleep(2)
        run_for_all(nodes_this_iteration, lambda node: [
            ssh(node, f"bash ~/scripts/start_{node.type}.sh")
        ])
    if print_uri:
        uri=lnk(f"http://{first('bench').public_ip}:8080"
                f"/secret={get_gui_secret()}/grafana/d/rdrs")
        print(f'Go to {uri} in your browser to see the dashboard.')

# stop command
def do_stop_cluster(for_nodes):
    run_for_all(for_nodes, lambda node: [
        ssh(node, "bash ~/scripts/cleanup.sh")
    ])

# open_tmux command
def do_open_tmux(for_nodes):
    if not shutil.which("tmux"):
        die("tmux is not installed.")
    def tmux(*args, check=True, **kwargs):
        cmd = ["tmux", *args]
        return subprocess.run(cmd, check=check, **kwargs).returncode == 0
    session_name = f"rondb_bm {get_unique_suffix()}"
    if not tmux("has-session", "-t", session_name, capture_output=True, check=False):
        tmux("new-session", "-d", "-s", session_name)
        for node in for_nodes:
            name = node.name()
            tmux("new-window", "-t", session_name, "-n", name, *ssh(node))
            line=f"cd {RUN_DIR}/{node.type}"
            if node.type == "bench":
                line=f"cd {RUN_DIR}"
            if node.type == "mysqld":
                line=f"cd {RUN_DIR}/{node.type}; alias mysql=\"{WORKSPACE}/rondb/bin/mysql -uroot\""
            tmux("send-keys", "-t", f"{session_name}:{name}", line, "C-m")
        tmux("kill-window", "-t", f"{session_name}:0")
    tmux("attach-session", "-t", session_name)

# ssh command
def do_ssh(nodename, *cmd):
    for node in all_nodes():
        if not node.name() == nodename: continue
        exit(subprocess.run(ssh(node, ' '.join(cmd))).returncode)
    die("Could not find a node by that name. Try ./cluster_ctl list")

# kill_tmux command
def do_kill_tmux():
    if not shutil.which("tmux"):
        die("tmux is not installed.")
    if not Path("secrets.json").exists():
        # Avoid creating secrets.json unnecessarily.
        print("No tmux session to delete.")
        return
    session_name = f"rondb_bm {get_unique_suffix()}"
    if subprocess.run(["tmux", "has-session", "-t", session_name],
                      capture_output=True).returncode != 0:
        print("No tmux session to delete.")
        return
    print(f"Deleting tmux session '{session_name}'.")
    subprocess.run(["tmux", "kill-session", "-t", session_name], check=True)
    print("Done deleting tmux session.")

# bench_locust command
def do_bench_locust(columns, rows, column_types, total_workers):
    do_start_locust(rows, total_workers, only_validate=True)
    do_populate(columns, rows, column_types)
    do_start_locust(rows, total_workers)

# populate command
def do_populate(columns, rows, column_types):
    # Validate parameters
    first('mysqld') # Assert that one exists
    if columns < 1 or 200 < columns:
        die("NUM_COLUMNS must be in the range 1 - 200")
    if rows < 1 or 1000000000 < rows:
        die("NUM_ROWS must be in the range 1 - 1,000,000,000")
    if column_types not in ['INT', 'VARCHAR', 'INT_AND_VARCHAR']:
        die("COLUMN_TYPES must be one of: INT, VARCHAR, INT_AND_VARCHAR")
    batch_size = max(math.floor(10000 / columns), 1)
    column_info = {'INT': 1, 'VARCHAR': 2, 'INT_AND_VARCHAR': 0}[column_types]
    # Populate table
    run_for_one_node(first('mysqld'), lambda node: [
        ssh(node, f"bash ~/scripts/populate_locust.sh {columns} {rows}" +
            f" {batch_size} {column_info}")
    ])
    check_failed()

# start_locust command
def do_start_locust(rows, total_workers, only_validate=False):
    # Validate parameters
    if rows < 1 or 1000000000 < rows:
        die("NUM_ROWS must be in the range 1 - 1,000,000,000")
    num_bench_nodes = node_count("bench")
    cpus_per_node = get_tf_output('bench_cpus_per_node')
    cpus = num_bench_nodes * cpus_per_node
    max_workers = cpus - 1 # Reserve one CPU for master
    min_workers = max(1, num_bench_nodes - 1)
    if max_workers < min_workers:
        die("Error: Not enough CPUs for workers.")
    if total_workers == None: total_workers = max_workers
    if total_workers < min_workers or max_workers < total_workers:
        die(f"Error: Total workers must be in the range {min_workers} - {max_workers}")
    if only_validate: return
    # Stop
    do_stop_locust()
    # Start
    def cmdfun(node):
        num_workers = math.floor(total_workers / num_bench_nodes)
        if ((total_workers - num_workers * num_bench_nodes) >=
            (num_bench_nodes - node.idx)):
            num_workers += 1
        return [ ssh(node, f"bash ~/scripts/start_locust.sh {rows}" +
                     f" {num_workers}") ]
    run_for_all(nodes_of_type("bench"), cmdfun)
    # Print URI
    uri=lnk(f"http://{first('bench').public_ip}:8080/secret={get_gui_secret()}/locust")
    print(f"Go to {uri} in your browser to start the benchmark")

# stop_locust command
def do_stop_locust():
    run_for_all(nodes_of_type("bench"), lambda node: [
        ssh(node, "source ~/scripts/include.sh && stop locust")
    ])

# start_demo command
def do_start_demo():
    run_for_one_node(first('bench'), lambda node: [
        ssh(node, "bash ~/scripts/start_python_server.sh")
    ])
    check_failed()
    uri=lnk(f"http://{first('bench').public_ip}:8080/")
    print(f'Go to {uri} in your browser to see the demo UI.')

# stop_demo command
def do_stop_demo():
    run_for_one_node(first('bench'), lambda node: [
        ssh(node, "source ~/scripts/include.sh && stop uvicorn")
    ])
    check_failed()

# list command
def do_list():
    run_for_all(all_nodes(), lambda _: [UPDATE_STATUS])
    rows = [[
        color("Name", "1;4", " "),
        color('Type', "1;4"),
        color('Public IP', "1;4"),
        color('Private IP', "1;4"),
        color('NDB Node IDs', "1;4"),
        color('Status', "1;4"),
    ]]
    for node in all_nodes():
        rows.append([
            node.tui_name(),
            node.type,
            node.public_ip,
            node.private_ip,
            ', '.join(map(str, node.ndb_nodeids)),
            node.status if node.status is not None else red('ERROR'),
        ])
    def width(s): return len(re.sub(r'\033\[[0-9;]*m', '', s))
    maxwidths = [ max(width(row[i]) for row in rows) for i in range(len(rows[0])) ]
    for row in rows:
        print('   '.join(s + ' ' * (maxwidths[i] - width(s))
                       for i, s in enumerate(row)).rstrip())

# cleanup command
def do_cleanup():
    # Destroy terraform cluster if it exists
    state_file = Path("terraform.tfstate")
    if state_file.exists():
        print("Destroying terraform cluster.")
        if subprocess.run(["terraform", "destroy", "-auto-approve"]).returncode != 0:
            die("'terraform destroy' failed.")
        print("Done destroying terraform cluster.")
    else:
        print("Skip 'terraform destroy' since there is no terraform.tfstate"
              " file.")
    # Destroy AWS SSH key pair if it exists
    key_name=get_key_name()
    key_file=Path(f"{key_name}.pem")
    if key_file.exists():
        print("Deleting AWS SSH key.")
        subprocess.run(["aws", "ec2", "delete-key-pair",
                        "--key-name", key_name,
                        "--region", get_config('region')],
                       check=True)
        key_file.unlink()
        print("Done deleting AWS SSH key.")
    else:
        print("No AWS SSH key to delete.")
    if shutil.which("tmux"):
        do_kill_tmux()
    print("Deleting temporary files.")
    subprocess.run("rm -rf .terraform* terraform.* tf_output config_files"
                   " __pycache__ secrets.json",
                   shell=True, check=True)
    print("Done deleting temporary files.")

# CLI
if __name__ == "__main__":
    argc = len(sys.argv)
    node_type_names = [x[0] for x in node_types]
    if argc < 2:
        print('\n'.join([
            "Usage:",
            "  ./cluster_ctl deploy [NODES]",
            "    Configure, initialize and apply terraform, as needed. Then install and start",
            "    RonDB services.",
            "      NODES: See bottom.",
            "",
            "  ./cluster_ctl terraform",
            "    Configure, initialize and apply terraform, as needed.",
            "",
            "  ./cluster_ctl install [NODES]",
            "    Install RonDB services without starting. Called by deploy.",
            "      NODES: See bottom.",
            "",
            "  ./cluster_ctl start [NODES]",
            "    Start or restart RonDB services, without installing. Called by deploy.",
            "      NODES: See bottom.",
            "",
            "  ./cluster_ctl stop [NODES]",
            "    Stop RonDB services. Called by start (and hence deploy).",
            "      NODES: See bottom.",
            "",
            "  ./cluster_ctl open_tmux [NODES]",
            "    Attach to a tmux session with ssh connections opened to all applicable",
            "    nodes. If a session does not exist, it is created according to the NODES",
            "    parameter, otherwise the parameter is ignored. To connect to a different set",
            "    of nodes, use kill_tmux first.",
            "      NODES: See bottom.",
            "",
            "  ./cluster_ctl kill_tmux",
            "    Kill the tmux session created by open_tmux.",
            "",
            "  ./cluster_ctl ssh NODE_NAME",
            "    Open an ssh connection to a node.",
            "",
            "  ./cluster_ctl bench_locust [--cols COLUMNS] [--rows ROWS]\\",
            "                             [--types COLUMN_TYPES] [--workers WORKERS]",
            "    Populate a table with test data and start locust ready to run a benchmark",
            "    against it.",
            "      COLUMNS: Number of columns in table. Default 10.",
            "      ROWS: Number of rows in table. Default 100000.",
            "      COLUMN_TYPES: The columns types in table; one of INT, VARCHAR,",
            "                    INT_AND_VARCHAR. Default INT.",
            "      WORKERS: Number of locust workers. Defaults to the maximum possible.",
            "",
            "  ./cluster_ctl populate [--cols COLUMNS] [--rows ROWS] [--types COLUMN_TYPES]",
            "    Populate a table with test data. Called by bench_locust.",
            "      COLUMNS: Number of columns in table. Default 10.",
            "      ROWS: Number of rows in table. Default 100000.",
            "      COLUMN_TYPES: The columns types in table; one of INT, VARCHAR,",
            "                    INT_AND_VARCHAR. Default INT.",
            "",
            "  ./cluster_ctl start_locust [--rows ROWS] [--workers WORKERS]\\",
            "    Start locust services. Called by bench_locust.",
            "      ROWS: Number of rows in table. Default 100000. MUST be set to the same",
            "            value as the last call to populate. Prefer using bench_locust, which",
            "            will do the right thing automatically.",
            "      WORKERS: Number of locust workers. Defaults to the maximum possible.",
            "",
            "  ./cluster_ctl stop_locust",
            "    Stop locust. Called by start_locust (and hence bench_locust).",
            "",
            "  ./cluster_ctl start_demo",
            "    Start demo web UI.",
            "",
            "  ./cluster_ctl stop_demo",
            "    Stop demo web UI.",
            "",
            "  ./cluster_ctl list",
            "    List node information.",
            "",
            "  ./cluster_ctl cleanup",
            "    Destroy AWS resources and delete temporary files and tmux session.",
            "",
            "NODES: A comma-separated list of node names and node types. If given, operate",
            "       only on matching nodes, otherwise on all nodes. Available node types are:",
            "       "+', '.join(node_type_names),
        ]))
        die("")
    action = sys.argv[1]
    if action in ["deploy", "install", "start", "stop", "open_tmux"]:
        arg = None
        if argc == 2: pass
        elif argc == 3:
            arg = sys.argv[2]
        else: die(f"Usage: ./cluster_ctl {action} [NODES]")
        if action != "deploy": arg = nodes_from_expression(arg)
        if action == "deploy": do_deploy(arg)
        elif action == "install": do_install(arg)
        elif action == "start": do_start_cluster(arg)
        elif action == "stop": do_stop_cluster(arg)
        elif action == "open_tmux": do_open_tmux(arg)
        else: die("Bug")
    elif action == "ssh":
        if argc < 3:
            die("Usage: ./cluster_ctl ssh NODE_NAME [COMMAND ...]")
        do_ssh(*sys.argv[2:])
    elif action in ["terraform",
                    "kill_tmux",
                    "stop_locust",
                    "start_demo",
                    "stop_demo",
                    "list",
                    "cleanup"]:
        if argc != 2:
            die(f"Usage: ./cluster_ctl {action}")
        if action == "terraform": do_terraform()
        elif action == "kill_tmux": do_kill_tmux()
        elif action == "stop_locust": do_stop_locust()
        elif action == "start_demo": do_start_demo()
        elif action == "stop_demo": do_stop_demo()
        elif action == "list": do_list()
        elif action == "cleanup": do_cleanup()
        else: die("Bug")
    elif action in ["bench_locust", "populate", "start_locust"]:
        columns = 10
        rows = 100000
        column_types = "INT"
        total_workers = None
        i = 2
        while i < argc:
            arg = sys.argv[i]
            if arg == "--cols" and action != "start_locust":
                i += 1
                if i >= argc: die("Missing value after --cols")
                columns = int(sys.argv[i])
            elif arg == "--rows":
                i += 1
                if i >= argc: die("Missing value after --rows")
                rows = int(sys.argv[i])
            elif arg == "--types" and action != "start_locust":
                i += 1
                if i >= argc: die("Missing value after --types")
                column_types = sys.argv[i]
            elif arg == "--workers" and action != "populate":
                i += 1
                if i >= argc: die("Missing value after --workers")
                total_workers = int(sys.argv[i])
            else:
                die(f"Unknown argument: {arg}")
            i += 1
        if action == "bench_locust":
            do_bench_locust(columns, rows, column_types, total_workers)
        elif action == "populate":
            do_populate(columns, rows, column_types)
        elif action == "start_locust":
            do_start_locust(rows, total_workers)
        else: die("Bug")
    else: die(f"Unknown action: {action}")
